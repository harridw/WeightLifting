---
title: "Machine Learning: Evaluate Weight Lifting Techniques"
output: html_document

---

```{r setup, include = FALSE, echo = TRUE, results = "hide"}
ipak <- function(pkg){
      new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
      if (length(new.pkg)) 
            install.packages(new.pkg, dependencies = TRUE)
      sapply(pkg, require, character.only = TRUE)
}

### Package Usage
packages <- c("plyr", "dplyr", "data.table", "dtplyr", "lubridate", "ggplot2", "scales",
                  "reshape2", "knitr", "R.cache", "stringr", "gtools", "quantreg",
                  "graphics", "corrplot", "broom", "rmarkdown", "caret", "randomForest",
                  "gbm", "forecast", "elasticnet", "e1071", "glmnet", "quantmod", "rpart",
                  "rpart.plot", "rattle", "knitr", "tidyverse", "purrr", "mda", "ROCR",
                  "verification", "pROC", "tree", "stringr", "cvTools")
ipak(packages)
```

## Executive Summary  
**Background** 
```r
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.  
```
**Analysis Goal**  
```r
In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here:  
```
<http://groupware.les.inf.puc-rio.br/har>  (see the section on the Weight Lifting Exercise Dataset). 

## Data Processing

**Load Data into R**  
```{r load_weightlift_csv, include = TRUE, echo = TRUE, results = "hold"}
train.file = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test.file = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

training <- read.csv(train.file, header = TRUE, 
                  na.strings = c("NA","N/A","NULL","Null","null",""," ","  ","#DIV/0!"))
testing <- read.csv(test.file, header = TRUE, 
                  na.strings = c("NA","N/A","NULL","Null","null",""," ","  ","#DIV/0!"))
dim(training)
dim(testing)
```


**Partition Training Data Set**  
To allow for a validation of the models to occur before evaluating predictive accuracy on test set, a validation set, is created.  Due to the limited size of the testing set, 20 observations, this validation set will afford another opportunity to evaluate the prediction model.  As defined in code below, the original training set is split 70% training/30% validation. 
```{r testing_partition, include = TRUE, echo = TRUE, results = "markup", cache = TRUE}
set.seed(100)
inTrain = createDataPartition(training$classe, p = 0.7, list = FALSE)
training = training[inTrain,]
validation = training[-inTrain,]
dim(training) 
dim(validation) 
```


## Exploratory Data Analysis  
There is a total of 160 variables in the data sets.  To avoid overfitting, we want to identify important or relevant variables, or features, that may contribute significantly to greater predictive capability of a model.   A first step is to identify and remove features with a percentage of observations values equal NA or blank is 80% or higher.  The assumption is that the high percentage of NA or blank values raises some concern about the quality or completeness of information obtained.  More research is required before further considering these variables for inclusion in the model. This may include better populating values across all observations.  
**Remove Variables with High Percentage 'NA'**   
```{r remove_NA, include = TRUE, echo = TRUE, results = "markup"}
pctNA <- sapply(training, function(x) round(sum(is.na(x))/nrow(training),3))
lowNA <- names(pctNA)[pctNA < 0.80]   ## Exclude column names with 80%+ NA
```

**Remove Unnecessary Variables**  
The analysis focuses on the data produced by the various accelerometers, not the individuals and/or time related to the data.  To avoid these items influencing our model, they are excluded here.
```{r remove_unrelated, include = TRUE, echo = TRUE, results = "markup"}
lowNA.remove <- lowNA[1:7]
lowNA.remove
```

```{r keep_variables_one, include = TRUE, echo = FALSE, results = "hide"}
keep <- lowNA[9:length(lowNA)-1]   ## Exclude row index and 'classe' variable
```

Variables evaluated to fit model:
```{r keep_variables_two, include = TRUE, echo = FALSE, results = "markup"}
keep
```
```{r nzv_exclude, include = FALSE, echo = FALSE, eval = FALSE}
## Identify nearZeroVar which likely contribute nominally, if at all, to predictive model
nzv <- nearZeroVar(training, freqCut = 80/20, uniqueCut = 0.1, saveMetrics = TRUE)
nzv.disq <- subset(nzv, zeroVar == TRUE | nzv == TRUE)
nzv.disq <- rownames(nzv.disq)
```


## Model Selection
As our outcome variable 'classe' is qualitative, a linear model does not make sense. We will be using the models listed below to predict the outcome.  

* Recursive Partitioning (rpart)  
* Generalized Boosted Regression (gbm)  
* randomForest (rf)  

A combined, or stacked, model was considered.  However, due to the results of these defined models, the 'potential' improvement in accuracy does not outweigh the loss of explanatory capability presented by the stacked model.  

```{r model_develop, include = TRUE, echo = TRUE, results = "hide", cache = TRUE}
set.seed(100)
fitCtrl <- trainControl(method = "cv", number = 5, search = "random", 
                        returnResamp = "all", savePredictions = "all",
                        classProbs = TRUE, selectionFunction = "best", trim = TRUE)
fitRPart <- train(classe ~ ., data = training[ , c("classe", keep)], method = "rpart", 
                  tuneLength = round(sqrt(ncol(training[,c("classe",keep)]))), 
                  trControl = fitCtrl, metric = "Accuracy")
predRPart <- predict(fitRPart, newdata = validation[ ,c("classe", keep)])
confuseRPart <- confusionMatrix(predRPart, validation$classe)

set.seed(100)
fitCtrl <- trainControl(method = "cv", number = 5, search = "random",
                        returnResamp = "all", savePredictions = "all",
                        classProbs = TRUE, selectionFunction = "best", trim = TRUE)
gbmGrid <- expand.grid(.interaction.depth = 1,
                   .n.trees = seq(10,50,by=10),
                   .shrinkage = 0.1, .n.minobsinnode=2)
fitGBM <- train(classe ~ ., data = training[ , c("classe", keep)], method = "gbm", 
                  tuneGrid = gbmGrid, trControl = fitCtrl,
                  metric = "Kappa", verbose = FALSE)
predGBM <- predict(fitGBM, newdata = validation[ ,c("classe", keep)])
confuseGBM <- confusionMatrix(predGBM, validation$classe)

set.seed(100)
fitCtrl <- trainControl(method = "oob", number = 5, search = "random",
                        returnResamp = "all", savePredictions = "all",
                        classProbs = TRUE, selectionFunction = "best", trim = TRUE)
mtry <- c(1:round(sqrt(ncol(training[,c("classe",keep)]))))
rfGrid <- data.frame(.mtry = mtry)
fitRF <- train(classe ~ ., data = training[ , c("classe", keep)], method = "rf", 
                  tuneGrid = rfGrid, trControl = fitCtrl, 
                  metric = "Accuracy", importance = FALSE)
predRF <- predict(fitRF, newdata = validation[ ,c("classe", keep)])
confuseRF <- confusionMatrix(predRF, validation$classe)
```


## Model Performance  
We use Accuracy as our measure of the performance or quality of a prediction.  Essentially, we calculate the number of correct predictions to the total number of predictions.  Measuring the 'Accuracy' on the training set is not appropriate.  We apply the model to the validation set (see results below) to evaluate the 'Accuracy' of the model.  

**Model Accuracy**  
```{r model_accuracy, include = TRUE, echo = TRUE, results = "markup"}
model.accuracy <- data.frame(
      Method = c("Rpart", "Boosting", "randomForest"),
      Accuracy = rbind(round(confuseRPart$overall[1],4), 
                        round(confuseGBM$overall[1],4), 
                        round(confuseRF$overall[1],4)))
model.accuracy
```


**Model Result Details**  
```{r model_details, include = TRUE, echo = TRUE, results = "markup"}
fitRPart$results[1:3]
fitGBM$results[4:6]
fitRF$results[1:3]
```


**Variable Importance**  
```{r variable_importance, include = TRUE, echo = TRUE, results = "hide"}
rpartImp <- varImp(fitRPart, scale = FALSE)
gbmImp <- varImp(fitGBM, scale = FALSE)
rfImp <- varImp(fitRF, scale = FALSE)
```

```{r plot_importance, fig.keep = "high", fig.show = "asis", fig.path = 'figure/'}
par(mfrow = c(1,3))
plot(rpartImp, top = 20, main = "Variable Importance - rpart")
plot(gbmImp, top = 20, main = "Variable Importance - gbm")
plot(rfImp, top = 20, main = "Variable Importance - rf")
```


## Predict on Test Set
Upon reviewing the accuracy results of the three models, it was clear that our randomForest (rf) model produced the most accurate results on the validation set.  We will not predict 'classe' on each observation of the testing set.  
```{r testing_outcomes, include = TRUE, echo = TRUE, results = "markup"}
predRF.test <- predict(fitRF, newdata = testing)
predRF.test.df <- data.frame(obs_id = testing$X, predOutcome = predRF.test)
predRF.test.df
```

